nanobot
=======

Model summary
-------------

- Approximate parameter count: ~4.8 million
- Model class: ~5M-parameter GPT-style model

Notes
-----

This repository contains a minimal project for the "nanobot" model. The README records the approximate size and class of the model. Update this file with usage instructions, licensing, or model weights as needed.


 NanoBot-DeepSeek — 26.8M Parameter MoE Model (Trained from Scratch)

Built a Mixture-of-Experts Transformer in PyTorch with:

• ~26.8M total parameters
• ~3.5–4M active per forward pass
• Top-2 routing, 8 experts per layer
• Fully trained from scratch

Clean, minimal, and focused on understanding sparse scaling.

